{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MOT17-02-SDP', 'MOT17-11-SDP', 'MOT17-04-SDP', 'MOT17-05-SDP', 'MOT17-09-DPM', 'MOT17-11-FRCNN', 'MOT17-10-DPM', 'MOT17-10-FRCNN', 'MOT17-09-SDP', 'MOT17-10-SDP', 'MOT17-11-DPM', 'MOT17-13-SDP', 'MOT17-02-DPM', 'MOT17-13-FRCNN', 'MOT17-02-FRCNN', 'MOT17-13-DPM', 'MOT17-04-DPM', 'MOT17-05-FRCNN', 'MOT17-09-FRCNN', 'MOT17-04-FRCNN', 'MOT17-05-DPM']\n"
     ]
    }
   ],
   "source": [
    "mot_dir = \"/home/viplab/Research_DK/Mamba-MOT/GMTracker/data/MOT17/train\"\n",
    "print(os.listdir(mot_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 4)\n",
      "token shape is :  (50, 4)\n",
      "tensor([[ 110.3233, -100.0952,  -56.5693,   42.5965],\n",
      "        [ 109.8223,  -99.1943,  -55.6777,   42.0264],\n",
      "        [ 108.6947,  -97.0546,  -55.0401,   41.2286],\n",
      "        [ 107.3791,  -95.3689,  -54.3434,   40.5082],\n",
      "        [ 106.2515,  -93.2291,  -53.7057,   39.7104],\n",
      "        [ 104.9358,  -91.5435,  -53.0090,   38.9899],\n",
      "        [  99.2570,  -93.3553,  -55.7706,   42.6067],\n",
      "        [  92.5880,  -94.7836,  -59.0507,   46.4234],\n",
      "        [  86.9092,  -96.5954,  -61.8122,   50.0402],\n",
      "        [  80.2402,  -98.0238,  -65.0923,   53.8569],\n",
      "        [  79.7711,  -96.2077,  -62.6003,   52.1179],\n",
      "        [  77.9544,  -93.6211,  -61.0119,   50.8274],\n",
      "        [  76.1378,  -91.0346,  -59.4235,   49.5369],\n",
      "        [  74.3211,  -88.4481,  -57.8352,   48.2463],\n",
      "        [  78.2724,  -79.1531,  -50.1997,   39.8168],\n",
      "        [  82.7566,  -69.8438,  -41.8555,   30.7886],\n",
      "        [  86.8835,  -60.1475,  -33.8964,   22.0088],\n",
      "        [  90.5655,  -50.0006,  -26.1299,   13.1031],\n",
      "        [  91.2430,  -48.5006,  -24.0701,   11.1566],\n",
      "        [  91.5509,  -46.6663,  -22.0126,    9.1857],\n",
      "        [  91.6831,  -45.2334,  -20.2788,    7.5650],\n",
      "        [  92.3482,  -43.7861,  -17.8362,    5.3456],\n",
      "        [  89.4211,  -44.6284,  -19.0244,    6.7795],\n",
      "        [  86.4940,  -45.4706,  -20.2127,    8.2134],\n",
      "        [  83.1973,  -45.9787,  -21.4033,    9.6230],\n",
      "        [  76.8359,  -46.1013,  -23.3347,   12.5679],\n",
      "        [  70.0295,  -45.7734,  -25.4586,   15.3869],\n",
      "        [  64.0254,  -46.2830,  -27.0049,   18.0834],\n",
      "        [  57.2190,  -45.9551,  -29.1288,   20.9024],\n",
      "        [  51.2149,  -46.4647,  -30.6751,   23.5988],\n",
      "        [  44.5841,  -45.7354,  -32.4754,   26.0676],\n",
      "        [  46.7923,  -40.8339,  -25.9723,   19.8777],\n",
      "        [  48.9882,  -35.9851,  -19.0865,   13.4149],\n",
      "        [  50.8267,  -30.7494,  -12.5858,    7.2007],\n",
      "        [  53.3922,  -26.2349,   -5.6975,    0.7623],\n",
      "        [  55.2307,  -20.9992,    0.8032,   -5.4520],\n",
      "        [  56.6243,  -15.3129,    7.1114,  -11.7922],\n",
      "        [  57.5588,  -14.7175,    9.4229,  -13.5354],\n",
      "        [  58.0483,  -13.6716,   11.5418,  -15.4046],\n",
      "        [  59.1583,  -12.6748,   14.1769,  -17.4981],\n",
      "        [  59.6478,  -11.6289,   16.2959,  -19.3672],\n",
      "        [  60.5823,  -11.0335,   18.6073,  -21.1105],\n",
      "        [  61.2474,   -9.5861,   21.0499,  -23.3299],\n",
      "        [  53.6388,   -8.4207,   18.3483,  -20.3884],\n",
      "        [  46.0302,   -7.2552,   15.6467,  -17.4468],\n",
      "        [  38.0519,   -5.7554,   12.9427,  -14.5297],\n",
      "        [  30.4433,   -4.5899,   10.2411,  -11.5881],\n",
      "        [  23.3676,   -3.4101,    8.2483,   -9.2454],\n",
      "        [  15.2137,   -2.3117,    5.2207,   -5.9780],\n",
      "        [   7.7807,   -0.7448,    2.8427,   -3.3867]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      " shape of embedded tensot  :  torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_tracklet(gt_in, object_id):\n",
    "    \"\"\"\n",
    "    Extract the tracklet for a given object ID.\n",
    "    :param df: DataFrame containing the ground truth data.\n",
    "    :param object_id: The ID of the object to extract the tracklet for.\n",
    "    :return: DataFrame containing the tracklet.\n",
    "    \"\"\"\n",
    "    tracklet  = []\n",
    "    # tracklet.append(i) [for i in gt_in if gt_in[i] == object_id]\n",
    "    for i in gt_in:\n",
    "        # print(i)\n",
    "        if i[1] == object_id:\n",
    "            tracklet.append(i[2:6])\n",
    "    return np.array(tracklet)\n",
    "\n",
    "def tokenizer_frames(tracking, sliding_window):\n",
    "    \n",
    "    token  = [] \n",
    "    for i in range(sliding_window):\n",
    "        current_bbox = tracking[i]\n",
    "        # print(\"bounding box at {} frame : {}\".format(i,current_bbox))\n",
    "        # print(\"tracklet at Nth place is : \", tracking[sliding_window])\n",
    "        delta = np.subtract(tracking[sliding_window], current_bbox)\n",
    "        # print(\"delta is : \", delta)\n",
    "        token.append(delta)\n",
    "    \n",
    "    return np.array(token)\n",
    "\n",
    "\n",
    "for sequence in os.listdir(mot_dir):\n",
    "    # seq = os.path.join(mot_dir, sequence)\n",
    "    image_dir = os.path.join(mot_dir, sequence, \"img1\")\n",
    "    gt_file = os.path.join(mot_dir, sequence, \"gt\", \"gt.txt\")\n",
    "    # print(gt_file)\n",
    "\n",
    "    gt_in = np.loadtxt(gt_file, delimiter = \",\")\n",
    "\n",
    "    # print(gt_in)\n",
    "\n",
    "    ## Lets define 1 tracklet first\n",
    "    object_id = 2 ## Tracklet with Object ID = 1\n",
    "    tracklet = get_tracklet(gt_in, object_id)\n",
    "    # print(\" tracklet is : \", gt_in[tracklet])\n",
    "    print(tracklet.shape)\n",
    "\n",
    "    sliding_window = 50\n",
    "    token = tokenizer_frames(tracklet, sliding_window)\n",
    "\n",
    "\n",
    "    print(\"token shape is : \", token.shape)\n",
    "    token = torch.tensor(token, dtype = torch.float32)\n",
    "\n",
    "    input_dim = 4\n",
    "    output_dim = 4\n",
    "    embedding_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    embedded_tensor = embedding_layer(token)\n",
    "\n",
    "    print(embedded_tensor)\n",
    "\n",
    "    print(\" shape of embedded tensot  : \", embedded_tensor.shape)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viplab/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "class BiMambaEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_state):\n",
    "        super(BiMambaEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.mamba = Mamba(d_model, n_state)\n",
    "\n",
    "        # Norm and feed-forward network layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection of the original input\n",
    "        residual = x\n",
    "        \n",
    "        # Forward Mamba\n",
    "        x_norm = self.norm1(x)\n",
    "        mamba_out_forward = self.mamba(x_norm)\n",
    "\n",
    "        # Backward Mamba\n",
    "        x_flip = torch.flip(x_norm, dims=[1])  # Flip Sequence\n",
    "        mamba_out_backward = self.mamba(x_flip)\n",
    "        mamba_out_backward = torch.flip(mamba_out_backward, dims=[1])  # Flip back\n",
    "\n",
    "        # Combining forward and backward\n",
    "        mamba_out = mamba_out_forward + mamba_out_backward\n",
    "        \n",
    "        mamba_out = self.norm2(mamba_out)\n",
    "        ff_out = self.feed_forward(mamba_out)\n",
    "\n",
    "        output = ff_out + residual\n",
    "        return output\n",
    "\n",
    "# Initialize and test the model\n",
    "d_model = 4\n",
    "n_state = 64\n",
    "model = BiMambaEncoder(d_model, n_state).cuda()\n",
    "x = torch.rand(1, 50, d_model).cuda()  # Analog input data: (batch_size, seq_len, feature_dim)\n",
    "output = model(x)\n",
    "print(output.shape)  # Mamba Out: (32, 100, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tracklets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tracklets(data):\n",
    "    first_frame = int(np.min(data[:, 0]))\n",
    "    first_frame_data = data[data[:, 0] == first_frame]\n",
    "    # print(\" first frame data : \", first_frame_data)\n",
    "\n",
    "    tracklets = {}\n",
    "    for row in first_frame_data:\n",
    "        # print(\" row is :\", row)\n",
    "        object_id = int(row[1])\n",
    "        if object_id not in tracklets:\n",
    "            tracklets[object_id] = [row[2:6]]\n",
    "    return tracklets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update tracklets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracklets_for_frame(data, tracklets, frame):\n",
    "    # unique_frames = np.unique(data[:, 0])\n",
    "\n",
    "\n",
    "    # for frame in unique_frames:\n",
    "    #     if frame == np.min(data[:, 0]):\n",
    "    #         continue\n",
    "    \n",
    "\n",
    "\n",
    "    frame_data = data[data[:, 0] == frame]\n",
    "\n",
    "    for row in frame_data:\n",
    "        object_id = int(row[1])\n",
    "        if object_id in tracklets:\n",
    "            tracklets[object_id].append(row[2:6])\n",
    "        else:\n",
    "            ## Initialize a new tracklet if the object ID does not exist\n",
    "            tracklets[object_id] = [row[2:6]]\n",
    "\n",
    "    return tracklets\n",
    "### Convert tracklet to numpy arrays\n",
    "\n",
    "def convert_tracklets_to_numpy(tracklets):\n",
    "    for object_id , tracklet_data in tracklets.items():\n",
    "        tracklets[object_id] = np.array(tracklet_data)\n",
    "    \n",
    "    return tracklets\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',', dtype=float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/viplab/Research_DK/Mamba-MOT/GMTracker/data/MOT17/train/MOT17-02-SDP/gt/gt.txt\"\n",
    "data = load_data(filename)\n",
    "tracklets = initialize_tracklets(data)\n",
    "\n",
    "unique_frames = np.unique(data[:, 0])\n",
    "for frame in unique_frames:\n",
    "    if frame == 1:\n",
    "        continue\n",
    "    else:\n",
    "        tracklets = update_tracklets_for_frame(data, tracklets, frame)\n",
    "    # trackletsupdate_tracklets(data, tracklets) ## Updates all the tracklets at once. Can be changed to sending data per frame and updating the tracklets in a better way\n",
    "tracklets = convert_tracklets_to_numpy(tracklets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracklets.get(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracklet Delta - Creating a Tokenizer function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 0 0]\n",
      " [5 5 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def compute_deltas(tracklet_data):\n",
    "    # Compute deltas as described\n",
    "    deltas = []\n",
    "    for i in range(len(tracklet_data) - 1):\n",
    "        delta = tracklet_data[i+1] - tracklet_data[i]\n",
    "        deltas.append(delta)\n",
    "    return np.array(deltas)\n",
    "\n",
    "\n",
    "# Example tracklet data: shape (num_frames, 4) with columns [cx, cy, w, h]\n",
    "tracklet_data = np.array([\n",
    "    [100, 150, 50, 60],\n",
    "    [105, 155, 50, 60],\n",
    "    [110, 160, 50, 60]\n",
    "])\n",
    "\n",
    "# Create delta values for input sequence\n",
    "deltas = compute_deltas(tracklet_data)\n",
    "print(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTokenEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(TemporalTokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# Parameters\n",
    "input_dim = deltas.shape[1]  # Number of features in deltas (e.g., 4)\n",
    "embedding_dim = 128  # Example embedding dimension\n",
    "print(input_dim)\n",
    "# Create and apply embedding layer\n",
    "# embedding_layer = TemporalTokenEmbedding(input_dim, embedding_dim)\n",
    "deltas_tensor = torch.tensor(deltas, dtype=torch.float32)\n",
    "# embedded_tokens = embedding_layer(deltas_tensor)\n",
    "# print(embedded_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to implement Bi-Mamba Encdoding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Forward and Backward Mamba Modules: \n",
    "Implement the forward and backward Mamba modules.\n",
    "#### Create a Bi-Mamba Block: \n",
    "Use the forward and backward modules to process input and apply normalization and MLP.\n",
    "#### Assemble the Bi-Mamba Encoding Layer: \n",
    "Stack multiple Bi-Mamba blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiMambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_state):\n",
    "        super(BiMambaBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.mamba = Mamba(d_model, n_state)\n",
    "\n",
    "        # Norm and feed-forward network layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection of the original input\n",
    "        residual = x\n",
    "        \n",
    "        # Forward Mamba\n",
    "        x_norm = self.norm1(x)\n",
    "        mamba_out_forward = self.mamba(x_norm)\n",
    "\n",
    "        # Backward Mamba\n",
    "        x_flip = torch.flip(x_norm, dims=[1])  # Flip Sequence\n",
    "        mamba_out_backward = self.mamba(x_flip)\n",
    "        mamba_out_backward = torch.flip(mamba_out_backward, dims=[1])  # Flip back\n",
    "\n",
    "        # Combining forward and backward\n",
    "        mamba_out = mamba_out_forward + mamba_out_backward\n",
    "        mamba_out1  = self.norm2(mamba_out)\n",
    "\n",
    "    \n",
    "        mamba_out2 = self.feed_forward(mamba_out)\n",
    "\n",
    "        ff_out  = mamba_out1 + mamba_out2\n",
    "        # output = ff_out + residual\n",
    "        return ff_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiMambaEncodingLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_blocks):\n",
    "        super(BiMambaEncodingLayer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        # self.blocks = nn.ModuleList([BiMambaBlock(input_dim, embedding_dim) for _ in range(num_blocks)])\n",
    "    \n",
    "        self.mamba_block = BiMambaBlock(embedding_dim, embedding_dim)\n",
    "    def forward(self, x):\n",
    "        # print(\" embedding dimension is : \", self.embedding_dim)\n",
    "        # for block in self.blocks:\n",
    "        x = self.mamba_block(x)\n",
    "        # x = self.mamba_block(x)\n",
    "        # x = self.mamba_block(x)\n",
    "        \n",
    "            # x = block(x)\n",
    "            # print(\" x shape in mamba block is : \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x shape is :  torch.Size([2, 4])\n",
      " x after reshaping it is :  torch.Size([1, 2, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected x.is_cuda() to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create and apply model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m FullModel(input_dim, embedding_dim, num_blocks, prediction_dim)\n\u001b[0;32m---> 26\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeltas_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m, in \u001b[0;36mFullModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x  \u001b[38;5;241m=\u001b[39m  x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m x after reshaping it is : \u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbi_mamba_encoding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m x shape after  bimamba encoding layer is : \u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_head(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m, in \u001b[0;36mBiMambaEncodingLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(\" embedding dimension is : \", self.embedding_dim)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# for block in self.blocks:\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# x = self.mamba_block(x)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# x = self.mamba_block(x)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# x = block(x)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# print(\" x shape in mamba block is : \", x.shape)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m, in \u001b[0;36mBiMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Forward Mamba\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m---> 23\u001b[0m mamba_out_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward Mamba\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x_flip \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(x_norm, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Flip Sequence\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/mamba_ssm/modules/mamba_simple.py:146\u001b[0m, in \u001b[0;36mMamba.forward\u001b[0;34m(self, hidden_states, inference_params)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# In the backward pass we write dx and dz next to each other to avoid torch.cat\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fast_path \u001b[38;5;129;01mand\u001b[39;00m inference_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Doesn't support outputting the states\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_inner_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input-dependent B\u001b[39;49;00m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input-dependent C\u001b[39;49;00m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta_softplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     x, z \u001b[38;5;241m=\u001b[39m xz\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:306\u001b[0m, in \u001b[0;36mmamba_inner_fn\u001b[0;34m(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmamba_inner_fn\u001b[39m(\n\u001b[1;32m    301\u001b[0m     xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n\u001b[1;32m    302\u001b[0m     out_proj_weight, out_proj_bias,\n\u001b[1;32m    303\u001b[0m     A, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, B_proj_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    304\u001b[0m     C_proj_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta_softplus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    305\u001b[0m ):\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMambaInnerFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mout_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_proj_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_proj_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_softplus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:113\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_fetrack/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:181\u001b[0m, in \u001b[0;36mMambaInnerFn.forward\u001b[0;34m(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus, checkpoint_lvl)\u001b[0m\n\u001b[1;32m    179\u001b[0m x, z \u001b[38;5;241m=\u001b[39m xz\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    180\u001b[0m conv1d_bias \u001b[38;5;241m=\u001b[39m conv1d_bias\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mif\u001b[39;00m conv1d_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m conv1d_out \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_conv1d_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_conv1d_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# We're being very careful here about the layout, to avoid extra transposes.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# We want delta to have d as the slowest moving dimension\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m x_dbl \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(rearrange(conv1d_out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb d l -> (b l) d\u001b[39m\u001b[38;5;124m'\u001b[39m), x_proj_weight)  \u001b[38;5;66;03m# (bl d)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected x.is_cuda() to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_blocks, prediction_dim):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.temporal_token_embedding = TemporalTokenEmbedding(input_dim, embedding_dim)\n",
    "        self.bi_mamba_encoding_layer = BiMambaEncodingLayer(embedding_dim, num_blocks)\n",
    "        self.prediction_head = nn.Linear(embedding_dim, prediction_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.temporal_token_embedding(x)\n",
    "        print(\" x shape is : \", x.shape)\n",
    "        print(' type of x is : ', type(x))\n",
    "        x  =  x.unsqueeze(0)\n",
    "        print(\" x after reshaping it is : \", x.shape)\n",
    "        x = self.bi_mamba_encoding_layer(x)\n",
    "        print(\" x shape after  bimamba encoding layer is : \", x.shape)\n",
    "        x = self.prediction_head(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = deltas_tensor.shape[1]  # Number of features in deltas (e.g., 4)\n",
    "embedding_dim = 4 # Example embedding dimension\n",
    "num_blocks = 4  # Number of Bi-Mamba blocks\n",
    "prediction_dim = 4  # Number of predicted offsets\n",
    "\n",
    "# Create and apply model\n",
    "model = FullModel(input_dim, embedding_dim, num_blocks, prediction_dim)\n",
    "predictions = model(deltas_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_fetrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
